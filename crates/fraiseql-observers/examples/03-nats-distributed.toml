# NATS Distributed Observer Configuration
# ==========================================
#
# This deployment uses NATS JetStream for distributed event sourcing:
# - NATS JetStream for at-least-once event delivery
# - Redis for deduplication (prevents duplicate processing)
# - Redis for action result caching (performance optimization)
# - PostgreSQL → NATS bridge (publishes events from change log)
#
# Best for:
# - Multiple observer workers (horizontal scaling)
# - High availability (worker failures tolerated)
# - High event volume (10000+ events/sec)
# - Geographic distribution
#
# Architecture:
# PostgreSQL → Bridge → NATS JetStream → Workers (this config)
# ↓
# Redis (dedup + cache)

# Example observer: Webhook for all order events
[[observers]]
entity = "Order"
event_type = "INSERT"

[[observers.actions]]
body_template = '''
{
  "event_id": "{{ event.id }}",
  "event_type": "{{ event.event_type }}",
  "entity": "{{ event.entity_type }}",
  "data": {{ event.data }}
}
'''
headers = {"Authorization" = "Bearer ${API_TOKEN}"}
type = "webhook"
url_env = "ANALYTICS_WEBHOOK_URL"

[observers.retry]
backoff_strategy = "exponential"
initial_delay_ms = 100
max_attempts = 3
max_delay_ms = 30000

# Example observer: Email on order shipped (deduplicated + cached)
[[observers]]
condition = "data.status == 'shipped'"
entity = "Order"
event_type = "UPDATE"

[[observers.actions]]
body_template = "Order #{{ data.id }} is on its way. Tracking: {{ data.tracking_number }}"
subject = "Your order has shipped!"
to_template = "{{ data.customer_email }}"
type = "email"

[observers.retry]
backoff_strategy = "exponential"
initial_delay_ms = 200
max_attempts = 5  # More retries for critical emails
max_delay_ms = 60000

# Performance features (all enabled)
[performance]
backlog_alert_threshold = 1000
# Runtime settings (tuned for high throughput)
channel_capacity = 2000  # Larger buffer for NATS bursts
concurrent_timeout_ms = 30000
enable_caching = true  # ✅ Cache expensive operations
enable_concurrent = true  # ✅ Parallel action execution
enable_dedup = true  # ✅ CRITICAL: Prevents duplicate processing with at-least-once NATS delivery
max_concurrency = 100  # Higher concurrency (worker is dedicated)
max_concurrent_actions = 20  # Higher concurrency for worker
overflow_policy = "drop"
shutdown_timeout = "60s"  # Longer timeout for graceful NATS shutdown

# Redis configuration (required for dedup + caching)
[redis]
cache_ttl_secs = 60
command_timeout_secs = 2
connect_timeout_secs = 5
dedup_window_secs = 300  # 5 minutes (matches JetStream)
pool_size = 20  # Higher pool for distributed workers
url = "redis://redis-cluster:6379"

# Transport configuration
[transport]
run_bridge = false  # Bridge runs separately
run_executors = true  # This worker processes events
transport = "nats"  # ✅ NATS JetStream transport

# NATS transport settings
[transport.nats]
consumer_name = "fraiseql_observer_worker_group_1"  # Workers with same name compete for messages
stream_name = "fraiseql_events"
subject_prefix = "fraiseql.mutation"
url = "nats://nats-1:4222,nats://nats-2:4222,nats://nats-3:4222"  # HA cluster

# JetStream configuration
[transport.nats.jetstream]
ack_wait_secs = 30  # Wait 30s for ack before redelivery
dedup_window_minutes = 5  # NATS-level deduplication window
max_age_days = 7  # Keep events for 7 days
max_bytes = 10_737_418_240  # 10 GB max
max_deliver = 3  # Retry up to 3 times before DLQ
max_msgs = 10_000_000  # 10M messages max
