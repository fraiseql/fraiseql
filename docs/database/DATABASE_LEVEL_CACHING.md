# Database-Level Caching in Rust-First Architecture

**Date**: 2025-10-16
**Context**: When Rust transformation is fast (0.5ms), database queries become the bottleneck

---

## 🎯 Core Insight: The Bottleneck Shifts

**Before Rust Optimization**:
```
DB Query: 0.5ms (20% of time)
Python Transform: 20ms (80% of time) ← BOTTLENECK
Total: 20.5ms

Optimization target: Transformation layer
```

**After Rust Optimization**:
```
DB Query: 0.5ms (50% of time) ← NEW BOTTLENECK
Rust Transform: 0.5ms (50% of time)
Total: 1ms

Optimization target: Database layer
```

**Key Finding**: With Rust, **database becomes the main bottleneck**. Database-level caching becomes more valuable!

---

## 📊 Database-Level Caching Strategies

### Strategy 1: PostgreSQL Built-in Caching (Always On)

**What PostgreSQL Already Does**:

```sql
-- Query plan cache
PREPARE get_user AS SELECT data FROM users WHERE id = $1;
EXECUTE get_user(1);  -- Uses cached plan

-- Buffer pool (shared_buffers)
-- Hot data stays in memory automatically
-- No configuration needed - PostgreSQL manages it
```

**Performance Impact**:
```
First query:  0.8ms (cold - load from disk)
Second query: 0.1ms (hot - in buffer pool)

10x speedup on hot data
```

**Configuration** (in `postgresql.conf`):
```conf
# Increase shared buffers for better caching
shared_buffers = 4GB  # 25% of RAM

# Increase effective cache size (helps query planner)
effective_cache_size = 12GB  # 75% of RAM

# Work memory for sorting/hashing
work_mem = 64MB
```

**Verdict**: ✅ **Always use** - Free performance, PostgreSQL manages it automatically

---

### Strategy 2: Generated JSONB Columns (Already Using)

**What We're Currently Doing**:

```sql
CREATE TABLE users (
    id INT PRIMARY KEY,
    first_name TEXT,
    last_name TEXT,
    email TEXT,

    -- Generated column (auto-updates on write)
    data JSONB GENERATED ALWAYS AS (
        jsonb_build_object(
            'id', id,
            'first_name', first_name,
            'last_name', last_name,
            'email', email,
            'user_posts', (
                SELECT jsonb_agg(...)
                FROM posts
                WHERE user_id = users.id
                LIMIT 10
            )
        )
    ) STORED
);
```

**Performance**:
```
Query: SELECT data FROM users WHERE id = 1;
Execution: 0.05ms (indexed lookup + JSONB retrieve)

Without generated column:
Query: SELECT user + embedded posts (subquery)
Execution: 2-5ms (JOIN + aggregation)

Speedup: 40-100x
```

**Verdict**: ✅ **Already optimal** - Generated columns are database-level caching done right

---

### Strategy 3: Materialized Views (For Aggregations)

**When Useful**:
Complex aggregations that are:
- Expensive to compute (>100ms)
- Updated infrequently (hourly/daily)
- Acceptable staleness

**Example Use Case**: Analytics Dashboard

```sql
-- Materialized view for dashboard stats
CREATE MATERIALIZED VIEW mv_dashboard_stats AS
SELECT
    (SELECT COUNT(*) FROM users) as total_users,
    (SELECT COUNT(*) FROM posts) as total_posts,
    (SELECT COUNT(*) FROM posts WHERE created_at > NOW() - INTERVAL '24 hours') as posts_today,
    (SELECT AVG(LENGTH(content)) FROM posts) as avg_post_length,
    jsonb_build_object(
        'top_users', (
            SELECT jsonb_agg(jsonb_build_object('id', id, 'name', name, 'post_count', post_count))
            FROM (
                SELECT u.id, u.name, COUNT(p.id) as post_count
                FROM users u
                LEFT JOIN posts p ON p.user_id = u.id
                GROUP BY u.id
                ORDER BY post_count DESC
                LIMIT 10
            ) top
        )
    ) as top_users
;

-- Index for fast refresh
CREATE UNIQUE INDEX ON mv_dashboard_stats ((1));

-- Refresh strategy (choose one)
REFRESH MATERIALIZED VIEW CONCURRENTLY mv_dashboard_stats;  -- Manual/cron
-- OR: Automatic refresh on write (trigger-based)
```

**Performance**:

| Approach | Query Time | Staleness | Use Case |
|----------|------------|-----------|----------|
| **Live query** | 150ms | 0ms | Real-time required |
| **Materialized view** | 0.5ms | Minutes-Hours | Analytics OK |
| **Generated column** | 0.1ms | 0ms | Simple aggregations |

**When to Use**:
- ✅ Complex aggregations (multiple JOINs, GROUP BY)
- ✅ Analytics/reporting queries
- ✅ Acceptable staleness (refresh every 5-60 minutes)
- ❌ Real-time requirements
- ❌ User-specific data (low hit rate)

**Rust-First Integration**:

```python
@fraiseql.type(sql_source="mv_dashboard_stats", jsonb_column="top_users")
class DashboardStats:
    total_users: int
    total_posts: int
    posts_today: int
    avg_post_length: float
    top_users: list[dict]

@fraiseql.query
async def dashboard(info) -> DashboardStats:
    """
    Query materialized view (0.5ms)
    Rust transforms top_users JSONB (0.3ms)
    Total: 0.8ms (vs 150ms live query)

    190x speedup!
    """
    repo = Repository(info.context["db"], info.context)
    return await repo.find_one("mv_dashboard_stats")

# Refresh strategy: Cron job
# */5 * * * * psql -c "REFRESH MATERIALIZED VIEW CONCURRENTLY mv_dashboard_stats"
```

**Verdict**: ✅ **Use selectively** for expensive aggregations with acceptable staleness

---

### Strategy 4: UNLOGGED Tables (Ephemeral Cache)

**What UNLOGGED Means**:
- Not written to WAL (Write-Ahead Log)
- 2-3x faster writes
- **Data lost on crash** (not durable)
- Perfect for cache data

**Use Case**: Query result cache in database

```sql
-- UNLOGGED table for caching query results
CREATE UNLOGGED TABLE query_cache (
    cache_key TEXT PRIMARY KEY,
    result JSONB NOT NULL,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    expires_at TIMESTAMPTZ NOT NULL
);

-- Index for expiration cleanup
CREATE INDEX idx_query_cache_expires ON query_cache(expires_at);

-- Cleanup function (run periodically)
CREATE OR REPLACE FUNCTION cleanup_expired_cache()
RETURNS void AS $$
    DELETE FROM query_cache WHERE expires_at < NOW();
$$ LANGUAGE sql;
```

**Usage Pattern**:

```python
async def cached_query(cache_key: str, ttl: int, query_fn):
    """Query with database-level caching"""

    # 1. Check cache
    result = await db.fetchrow(
        "SELECT result FROM query_cache WHERE cache_key = $1 AND expires_at > NOW()",
        cache_key
    )

    if result:
        # Cache hit (0.1ms)
        return result['result']

    # 2. Execute query
    data = await query_fn()

    # 3. Store in cache
    await db.execute(
        """
        INSERT INTO query_cache (cache_key, result, expires_at)
        VALUES ($1, $2, NOW() + $3 * INTERVAL '1 second')
        ON CONFLICT (cache_key) DO UPDATE
        SET result = EXCLUDED.result, expires_at = EXCLUDED.expires_at
        """,
        cache_key, json.dumps(data), ttl
    )

    return data

# Usage
@fraiseql.query
async def expensive_query(info) -> DashboardStats:
    return await cached_query(
        cache_key="dashboard:main",
        ttl=300,  # 5 minutes
        query_fn=lambda: execute_expensive_query()
    )
```

**Performance Comparison**:

| Storage | Write Speed | Read Speed | Durability | Use Case |
|---------|-------------|------------|------------|----------|
| **Redis** | 0.2ms | 0.2ms | Optional | Distributed cache |
| **UNLOGGED table** | 0.15ms | 0.1ms | None | Local cache |
| **Regular table** | 0.4ms | 0.1ms | Full | Persistent data |

**Advantages**:
- ✅ Same database (no Redis needed)
- ✅ ACID transactions with cache
- ✅ SQL querying of cache
- ✅ Simpler infrastructure

**Disadvantages**:
- ❌ Lost on crash (acceptable for cache)
- ❌ Not distributed (per-database)
- ❌ Cleanup needed (TTL handling)

**Verdict**: ⚠️ **Use if avoiding Redis** - Good alternative for single-server deployments

---

### Strategy 5: Partial Indexes (Query-Specific Optimization)

**Concept**: Index only frequently-queried subsets

```sql
-- Instead of indexing all users
CREATE INDEX idx_users_all ON users(id);  -- 1GB index

-- Index only active users (90% of queries)
CREATE INDEX idx_users_active ON users(id)
WHERE active = true AND deleted_at IS NULL;  -- 100MB index

-- Query (uses smaller, faster index)
SELECT data FROM users WHERE id = 123 AND active = true AND deleted_at IS NULL;
```

**Performance**:

| Index Type | Size | Query Time | Use Case |
|------------|------|------------|----------|
| **Full index** | 1GB | 0.15ms | All data |
| **Partial index** | 100MB | 0.05ms | Common queries |

**More Examples**:

```sql
-- Index recent posts only (dashboard queries)
CREATE INDEX idx_posts_recent ON posts(created_at DESC)
WHERE created_at > NOW() - INTERVAL '30 days';

-- Index popular users only (profile page)
CREATE INDEX idx_users_popular ON users(id)
WHERE follower_count > 1000;

-- Index JSONB field for specific queries
CREATE INDEX idx_users_premium ON users(id)
WHERE (data->>'subscription_tier') = 'premium';
```

**Rust-First Integration**:

```python
@fraiseql.query
async def active_users(info, limit: int = 10) -> list[User]:
    """
    Uses partial index automatically
    Query planner chooses idx_users_active
    0.05ms vs 0.15ms (3x faster)
    """
    repo = Repository(info.context["db"], info.context)
    return await repo.find(
        "users",
        where={"active": True, "deleted_at": None},
        limit=limit
    )
```

**Verdict**: ✅ **Use for common query patterns** - Small indexes, faster queries

---

### Strategy 6: Result Cache Table (Manual Management)

**Concept**: Store pre-computed results as JSONB

```sql
-- Cache table for expensive queries
CREATE TABLE result_cache (
    cache_key TEXT PRIMARY KEY,
    query_type TEXT NOT NULL,  -- 'dashboard', 'report', etc.
    result JSONB NOT NULL,
    computed_at TIMESTAMPTZ DEFAULT NOW(),
    valid_until TIMESTAMPTZ NOT NULL,
    computation_time_ms INT,  -- Track how expensive it was
    hit_count INT DEFAULT 0   -- Track cache effectiveness
);

-- Indexes
CREATE INDEX idx_result_cache_type ON result_cache(query_type);
CREATE INDEX idx_result_cache_valid ON result_cache(valid_until);

-- Update hit count
CREATE OR REPLACE FUNCTION increment_cache_hit(key TEXT)
RETURNS void AS $$
    UPDATE result_cache SET hit_count = hit_count + 1 WHERE cache_key = key;
$$ LANGUAGE sql;
```

**Usage Pattern**:

```python
class DatabaseCache:
    """Database-level result cache with metrics"""

    async def get_or_compute(
        self,
        cache_key: str,
        query_type: str,
        ttl: int,
        compute_fn
    ) -> Any:
        # 1. Try cache
        cached = await self.db.fetchrow(
            """
            SELECT result, hit_count
            FROM result_cache
            WHERE cache_key = $1 AND valid_until > NOW()
            """,
            cache_key
        )

        if cached:
            # Cache hit - increment counter
            await self.db.execute(
                "SELECT increment_cache_hit($1)",
                cache_key
            )
            return json.loads(cached['result'])

        # 2. Cache miss - compute
        start = time.perf_counter()
        result = await compute_fn()
        duration_ms = (time.perf_counter() - start) * 1000

        # 3. Store with metrics
        await self.db.execute(
            """
            INSERT INTO result_cache (cache_key, query_type, result, valid_until, computation_time_ms)
            VALUES ($1, $2, $3, NOW() + $4 * INTERVAL '1 second', $5)
            ON CONFLICT (cache_key) DO UPDATE
            SET result = EXCLUDED.result,
                valid_until = EXCLUDED.valid_until,
                computation_time_ms = EXCLUDED.computation_time_ms,
                computed_at = NOW()
            """,
            cache_key, query_type, json.dumps(result), ttl, int(duration_ms)
        )

        return result

    async def get_cache_stats(self, query_type: str) -> dict:
        """Analyze cache effectiveness"""
        stats = await self.db.fetchrow(
            """
            SELECT
                COUNT(*) as total_entries,
                SUM(hit_count) as total_hits,
                AVG(computation_time_ms) as avg_computation_ms,
                SUM(CASE WHEN hit_count > 0 THEN 1 ELSE 0 END) as entries_with_hits
            FROM result_cache
            WHERE query_type = $1
            """,
            query_type
        )
        return dict(stats)

# Usage
@fraiseql.query
async def dashboard(info) -> Dashboard:
    cache = DatabaseCache(info.context["db"])

    return await cache.get_or_compute(
        cache_key="dashboard:main",
        query_type="dashboard",
        ttl=300,
        compute_fn=lambda: compute_expensive_dashboard()
    )

# Monitoring
async def analyze_cache_performance():
    stats = await cache.get_cache_stats("dashboard")
    print(f"Dashboard cache: {stats['total_hits']} hits, "
          f"avg computation: {stats['avg_computation_ms']}ms")
```

**Benefits**:
- ✅ Transaction safety (cache + data in same transaction)
- ✅ Built-in metrics (hit count, computation time)
- ✅ SQL querying of cache state
- ✅ No external dependencies

**Verdict**: ⚠️ **Use for complex scenarios** - More control than Redis, but more to manage

---

## 📊 Comparative Analysis

### Performance Comparison

| Strategy | Query Time | Setup Complexity | Maintenance | Best For |
|----------|------------|------------------|-------------|----------|
| **PostgreSQL built-in** | 0.1-0.5ms | None | None | Always use |
| **Generated columns** | 0.05-0.1ms | Low | None | Pre-computed data |
| **Materialized views** | 0.1-0.5ms | Medium | Refresh needed | Aggregations |
| **UNLOGGED tables** | 0.1-0.15ms | Low | Cleanup needed | Local cache |
| **Partial indexes** | 0.05ms | Low | None | Common queries |
| **Result cache table** | 0.1-0.2ms | Medium | Cleanup + metrics | Complex caching |
| **Redis (comparison)** | 0.2ms | High | External service | Distributed cache |

### When to Use Each Strategy

```
Decision Tree:

Is query slow (>10ms)?
├─ NO → Use PostgreSQL built-in + partial indexes
└─ YES → Continue

    Is it a complex aggregation?
    ├─ YES → Use materialized view
    └─ NO → Continue

        Is staleness acceptable?
        ├─ YES → Use result cache table or UNLOGGED table
        └─ NO → Optimize query (indexes, generated columns)

            Still slow?
            └─ Consider Redis or application-level caching
```

---

## 🎯 Recommended Setup for Rust-First Architecture

### Baseline (90% of use cases)

```sql
-- 1. PostgreSQL configuration (postgresql.conf)
shared_buffers = 4GB
effective_cache_size = 12GB
work_mem = 64MB

-- 2. Generated JSONB columns (already using)
CREATE TABLE users (
    id INT PRIMARY KEY,
    first_name TEXT,
    data JSONB GENERATED ALWAYS AS (...) STORED
);

-- 3. Partial indexes for common queries
CREATE INDEX idx_users_active ON users(id)
WHERE active = true AND deleted_at IS NULL;

-- 4. GIN index for JSONB queries
CREATE INDEX idx_users_data_gin ON users USING gin(data);
```

**Result**: 1-2ms queries for most operations

### Advanced (High-traffic APIs)

```sql
-- Add materialized views for dashboards
CREATE MATERIALIZED VIEW mv_dashboard_stats AS
SELECT ... complex aggregation ...;

-- Refresh every 5 minutes (cron)
*/5 * * * * psql -c "REFRESH MATERIALIZED VIEW CONCURRENTLY mv_dashboard_stats"

-- Add UNLOGGED cache table for query results
CREATE UNLOGGED TABLE query_cache (
    cache_key TEXT PRIMARY KEY,
    result JSONB,
    expires_at TIMESTAMPTZ
);

-- Cleanup every hour
0 * * * * psql -c "DELETE FROM query_cache WHERE expires_at < NOW()"
```

**Result**: 0.5-1ms for cached queries, <5ms for most queries

---

## 💡 Rust-First Architecture + Database Caching

### Optimal Stack

```
┌─────────────────────────────────────────────────────────┐
│ 1. PostgreSQL Configuration                             │
│    - Buffer pool (hot data in memory)                   │
│    - Query plan cache (fast repeated queries)           │
│    Benefit: 10x speedup on hot data                     │
└─────────────────┬───────────────────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────────────────┐
│ 2. Schema Optimization                                  │
│    - Generated JSONB columns (pre-computed)             │
│    - Partial indexes (smaller, faster)                  │
│    - GIN indexes for JSONB (fast lookups)               │
│    Benefit: 40-100x speedup for pre-computed data       │
└─────────────────┬───────────────────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────────────────┐
│ 3. Materialized Views (for aggregations)               │
│    - Complex aggregations pre-computed                  │
│    - Refresh strategy (cron/trigger)                    │
│    Benefit: 100-1000x speedup for analytics             │
└─────────────────┬───────────────────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────────────────┐
│ 4. Rust Transformation (always fast)                   │
│    - Snake_case → camelCase (0.5ms)                    │
│    - Field selection (0.1ms)                           │
│    - __typename injection (0.05ms)                     │
│    Benefit: 20x faster than Python                     │
└─────────────────┬───────────────────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────────────────┐
│ 5. Optional: Query Result Cache                        │
│    - UNLOGGED table or Redis                           │
│    - For very expensive queries only                   │
│    Benefit: 100x for cached expensive queries          │
└─────────────────────────────────────────────────────────┘
```

### Performance by Query Type

| Query Type | DB Strategy | Total Time | vs No Optimization |
|------------|-------------|------------|---------------------|
| **Simple lookup** | Generated column + partial index | 0.1ms | 5x |
| **List query** | Generated column + GIN index | 0.5ms | 10x |
| **Dashboard** | Materialized view | 0.5ms | 300x (was 150ms) |
| **Analytics** | Materialized view + cache | 0.3ms | 500x |

---

## 🚀 Implementation Example

### Schema Setup

```sql
-- users table with optimizations
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    first_name TEXT NOT NULL,
    last_name TEXT NOT NULL,
    email TEXT NOT NULL UNIQUE,
    active BOOLEAN DEFAULT true,
    deleted_at TIMESTAMPTZ,
    created_at TIMESTAMPTZ DEFAULT NOW(),

    -- Generated JSONB column (database-level caching)
    data JSONB GENERATED ALWAYS AS (
        jsonb_build_object(
            'id', id,
            'first_name', first_name,
            'last_name', last_name,
            'email', email,
            'active', active,
            'created_at', created_at,
            'user_posts', (
                SELECT jsonb_agg(
                    jsonb_build_object(
                        'id', p.id,
                        'title', p.title,
                        'created_at', p.created_at
                    )
                    ORDER BY p.created_at DESC
                )
                FROM posts p
                WHERE p.user_id = users.id AND p.deleted_at IS NULL
                LIMIT 10
            )
        )
    ) STORED
);

-- Optimized indexes
CREATE INDEX idx_users_active ON users(id) WHERE active = true AND deleted_at IS NULL;
CREATE INDEX idx_users_data_gin ON users USING gin(data);

-- Materialized view for dashboard
CREATE MATERIALIZED VIEW mv_dashboard AS
SELECT
    COUNT(*) as total_users,
    COUNT(*) FILTER (WHERE created_at > NOW() - INTERVAL '7 days') as new_users_week,
    jsonb_build_object(
        'top_users', (
            SELECT jsonb_agg(jsonb_build_object('id', id, 'name', first_name, 'posts', post_count))
            FROM (
                SELECT u.id, u.first_name, COUNT(p.id) as post_count
                FROM users u
                LEFT JOIN posts p ON p.user_id = u.id
                GROUP BY u.id
                ORDER BY post_count DESC
                LIMIT 10
            ) t
        )
    ) as stats
;

-- Refresh every 5 minutes
CREATE INDEX ON mv_dashboard ((1));  -- Needed for CONCURRENT refresh
```

### FraiseQL Integration

```python
import fraiseql
from fraiseql.repositories import Repository

@fraiseql.type(sql_source="users", jsonb_column="data")
class User:
    id: int
    first_name: str
    last_name: str
    email: str
    active: bool
    user_posts: list[Post] | None = None

@fraiseql.type(sql_source="mv_dashboard")
class Dashboard:
    total_users: int
    new_users_week: int
    stats: dict

# Simple query - uses generated column
@fraiseql.query
async def user(info, id: int) -> User:
    """
    Pipeline:
    1. SELECT data FROM users WHERE id = $1 (0.05ms - partial index)
    2. Rust transform (0.5ms)
    Total: 0.55ms
    """
    repo = Repository(info.context["db"], info.context)
    return await repo.find_one("users", id=id)

# Dashboard - uses materialized view
@fraiseql.query
async def dashboard(info) -> Dashboard:
    """
    Pipeline:
    1. SELECT * FROM mv_dashboard (0.1ms - cached)
    2. Rust transform (0.3ms)
    Total: 0.4ms (vs 150ms without MV!)

    375x speedup!
    """
    repo = Repository(info.context["db"], info.context)
    result = await repo.db.fetchrow("SELECT * FROM mv_dashboard")
    return fraiseql_rs.transform_one(result, "Dashboard", info)
```

---

## 🎯 Key Takeaways

### 1. Database Caching is MORE Valuable with Rust

**Why**: Rust makes transformation fast (0.5ms), so database becomes the bottleneck
- Without Rust: 80% time in transformation → optimize transformation
- With Rust: 50% time in database → optimize database

### 2. Generated Columns are Ideal

**Why**:
- ✅ Automatic (no refresh needed)
- ✅ Always up-to-date (updated on write)
- ✅ Fast (0.05ms lookup)
- ✅ Standard SQL (no special tooling)

**We're already using them!** This is the right approach.

### 3. Materialized Views for Aggregations

**Use when**:
- Complex aggregations (GROUP BY, multiple JOINs)
- Acceptable staleness (minutes to hours)
- Read-heavy (many queries per update)

**Performance**: 100-1000x speedup for complex analytics

### 4. Skip Redis for Most Cases

**Rust-first changes the equation**:
- Before: Redis needed because transformation is slow
- After: Database + Rust is fast enough (<2ms)

**Use Redis only if**:
- Distributed cache needed (multiple servers)
- Very high traffic (>10k RPS)
- Sub-millisecond latency required

### 5. PostgreSQL Configuration Matters

**Simple config changes**:
```conf
shared_buffers = 4GB
effective_cache_size = 12GB
work_mem = 64MB
```

**Impact**: 10x speedup on hot data (in buffer pool)

---

## 📋 Decision Matrix

| Scenario | DB Strategy | Expected Performance | Maintenance |
|----------|-------------|---------------------|-------------|
| **Simple lookup** | Generated column + partial index | 0.1ms | None |
| **List with filters** | Generated column + GIN index | 0.5ms | None |
| **Complex aggregation** | Materialized view | 0.5ms | Refresh (cron) |
| **Real-time analytics** | Optimize query + indexes | 5-10ms | Monitor slow queries |
| **Expensive query** | Result cache table | 0.2ms | Cleanup (cron) |

---

## 🚀 Summary

**Yes, database-level caching is VERY useful in Rust-first architecture!**

**Why**: Rust eliminates transformation bottleneck, making database optimization more impactful

**Best strategies**:
1. ✅ **PostgreSQL configuration** (always do this)
2. ✅ **Generated JSONB columns** (already using - optimal!)
3. ✅ **Partial indexes** (for common queries)
4. ✅ **Materialized views** (for aggregations)
5. ⚠️ **UNLOGGED tables** (if avoiding Redis)

**Skip**:
- ❌ Redis (for most cases - database is fast enough)
- ❌ Complex cache invalidation (use generated columns instead)

**Result**: 0.5-2ms for simple queries, 0.5-1ms for complex queries (with MVs)
