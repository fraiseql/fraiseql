# Phase 10: Production Hardening Roadmap

**Status**: Planning ‚Üí In Progress
**Total Effort**: 3-4 weeks (14-20 implementation days)
**Target**: Production-ready FraiseQL v2 with enterprise features

---

## Overview

Phase 10 transforms FraiseQL v2 from a functionally complete system into a production-grade platform suitable for enterprise deployments. It covers security, reliability, scalability, and operational excellence.

---

## Phase 10.1: Admission Control, Rate Limiting & Backpressure (3 days)

### Objective
Prevent resource exhaustion and ensure fair sharing of system capacity.

### Components

#### 1.1 Token Bucket Rate Limiter
**Files**: `crates/fraiseql-server/src/rate_limit.rs` (NEW, ~200 lines)

```rust
pub struct RateLimiter {
    // Per-client rate limiting
    capacity: u32,                    // Max tokens per window
    refill_rate: u32,                 // Tokens per second
    windows: Arc<Mutex<HashMap<ClientId, TokenBucket>>>,
}

impl RateLimiter {
    pub fn new(capacity: u32, refill_rate: u32) -> Self { /* */ }
    pub async fn acquire(&self, client_id: &str, tokens: u32) -> Result<()> { /* */ }
}
```

**Features**:
- Per-client rate limiting (enforce max QPS)
- Per-action rate limiting (webhook calls, Slack messages, emails)
- Separate limits for read vs write operations
- Sliding window algorithm (more accurate than fixed windows)
- Graceful degradation (queue requests instead of rejecting immediately)

**Configuration**:
```toml
[rate_limiting]
# Global limits
global_max_qps = 10000
global_max_concurrent = 100

# Per-client limits
client_max_qps = 100
client_max_concurrent = 10

# Per-action limits
webhook_max_per_sec = 50
slack_max_per_sec = 10
email_max_per_sec = 5
```

#### 1.2 Admission Control
**Files**: `crates/fraiseql-server/src/admission.rs` (NEW, ~150 lines)

**Features**:
- Queue depth monitoring (reject if queue > threshold)
- Memory usage limits (GC or reject if >80%)
- Database connection pool saturation (queue if at capacity)
- Graceful rejection with "try again later" HTTP 429

```rust
pub struct AdmissionController {
    max_queue_depth: usize,
    max_memory_bytes: usize,
    reject_if_saturated: bool,
}

impl AdmissionController {
    pub async fn check_admission(&self, event: &EntityEvent) -> Result<()> {
        // Check queue depth
        if job_queue.depth() > self.max_queue_depth {
            return Err(RateLimitError::QueueFull.into());
        }
        // Check memory
        if memory_usage() > self.max_memory_bytes {
            return Err(RateLimitError::OutOfMemory.into());
        }
        Ok(())
    }
}
```

#### 1.3 Backpressure Handling
**Files**: Modified `crates/fraiseql-observers/src/queued_executor.rs` (~50 lines)

**Features**:
- Queue rejection policies (fail-fast vs queue):
  - `FailFast`: Reject if queue full (fast but loses events)
  - `QueueAll`: Always queue (buffers memory if consumers slow)
  - `Adaptive`: Switch based on memory pressure
- Exponential backoff for failed queuing attempts
- Metrics for backpressure events

#### 1.4 Configuration
**Files**: Modified `crates/fraiseql-observers/src/config.rs` (~30 lines)

```toml
[rate_limiting]
enabled = true
global_qps = 10000
per_client_qps = 100
token_refill_rate = 100

[backpressure]
queue_rejection_policy = "adaptive"  # or "fail-fast", "queue-all"
max_queue_memory_mb = 500
memory_threshold_percent = 80
```

### Tests
- Unit: Rate limiter token calculation, window sliding
- Integration: Concurrent requests, backpressure triggering
- Load: 10k QPS burst handling

### Verification
- [ ] `cargo clippy` clean
- [ ] `cargo test rate_limit*` passes
- [ ] Load test: 10k QPS for 60 seconds, no rejections
- [ ] Memory test: Backpressure kicks in at >80% memory

---

## Phase 10.2: Deployment Patterns (2 days)

### Objective
Document and automate deployment to Kubernetes, Docker, systemd.

### 2.1 Kubernetes Manifests
**Files**: `k8s/` directory (NEW, ~400 lines)

```yaml
# k8s/fraiseql-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fraiseql-server
spec:
  replicas: 3  # HA setup
  template:
    spec:
      containers:
      - name: fraiseql-server
        image: fraiseql/fraiseql-server:latest
        env:
        - name: FRAISEQL_DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: fraiseql-secrets
              key: database-url
        ports:
        - containerPort: 8000  # GraphQL
        - containerPort: 5432  # Arrow Flight
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "2000m"

---
# k8s/fraiseql-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: fraiseql-service
spec:
  ports:
  - name: graphql
    port: 8000
    targetPort: 8000
  - name: arrow-flight
    port: 5432
    targetPort: 5432
  selector:
    app: fraiseql-server
  type: LoadBalancer  # Or NodePort for internal

---
# k8s/fraiseql-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fraiseql-config
data:
  fraiseql.toml: |
    [server]
    bind_address = "0.0.0.0:8000"
    [observer]
    max_workers = 10
    [rate_limiting]
    global_qps = 10000
```

### 2.2 Docker Compose (Development)
**Files**: `docker-compose.yml` (UPDATED, includes all services)

```yaml
version: '3.8'
services:
  fraiseql-server:
    build: .
    ports:
      - "8000:8000"  # GraphQL
      - "5432:5432"  # Arrow Flight
    environment:
      DATABASE_URL: postgres://postgres:password@postgres:5432/fraiseql
      REDIS_URL: redis://redis:6379
      NATS_URL: nats://nats:4222
    depends_on:
      - postgres
      - redis
      - nats
      - clickhouse
      - elasticsearch

  postgres:
    image: postgres:16
    environment:
      POSTGRES_DB: fraiseql
      POSTGRES_PASSWORD: password

  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes

  nats:
    image: nats:2.10-alpine
    command: -js

  clickhouse:
    image: clickhouse/clickhouse-server:latest
    ports:
      - "8123:8123"

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.0.0
    environment:
      xpack.security.enabled: "false"
```

### 2.3 Systemd Service (Single-Node)
**Files**: `systemd/fraiseql.service` (NEW)

```ini
[Unit]
Description=FraiseQL v2 GraphQL Server
After=network.target postgresql.service redis.service

[Service]
Type=simple
User=fraiseql
WorkingDirectory=/opt/fraiseql
ExecStart=/usr/local/bin/fraiseql-server --config /etc/fraiseql/fraiseql.toml
Restart=on-failure
RestartSec=10s

[Install]
WantedBy=multi-user.target
```

### 2.4 Terraform/CloudFormation
**Files**: `infrastructure/` directory (NEW, ~500 lines)

- RDS PostgreSQL (managed, automated backups)
- ElastiCache Redis (cluster mode)
- Elasticsearch managed service
- EKS cluster definition
- Load balancer setup

### Tests
- [ ] Docker image builds and runs
- [ ] K8s manifests are valid (`kubectl apply --dry-run`)
- [ ] Healthchecks respond correctly
- [ ] Service discovery works

### Verification
- [ ] Docker: `docker-compose up` works
- [ ] Kubernetes: 3 replicas healthy
- [ ] Systemd: Service starts and stays running

---

## Phase 10.3: Circuit Breakers, Retry Logic & Graceful Degradation (3 days)

### Objective
Handle transient failures without cascading, degrade gracefully under load.

### 3.1 Circuit Breaker Pattern
**Files**: `crates/fraiseql-observers/src/circuit_breaker.rs` (NEW, ~250 lines)

```rust
pub enum CircuitState {
    Closed,     // Normal operation
    Open,       // Failing, reject requests
    HalfOpen,   // Testing if service recovered
}

pub struct CircuitBreaker {
    state: Arc<Mutex<CircuitState>>,
    failure_threshold: u32,
    success_threshold: u32,
    timeout: Duration,
}

impl CircuitBreaker {
    pub async fn execute<F, T>(&self, f: F) -> Result<T>
    where
        F: Fn() -> BoxFuture<'static, Result<T>>,
    {
        match self.state() {
            CircuitState::Closed => {
                match f().await {
                    Ok(result) => Ok(result),
                    Err(e) => {
                        self.record_failure();
                        Err(e)
                    }
                }
            }
            CircuitState::Open => Err(CircuitBreakerOpen.into()),
            CircuitState::HalfOpen => {
                // Try one request to see if service recovered
                f().await
            }
        }
    }
}
```

**Per-action circuit breakers**:
- Webhook action: threshold=5 failures in 60s ‚Üí open
- Slack action: threshold=3 failures in 60s ‚Üí open
- Email action: threshold=2 failures in 60s ‚Üí open
- Database queries: threshold=10 failures ‚Üí open

### 3.2 Enhanced Retry Logic
**Files**: Modified `crates/fraiseql-observers/src/job_queue/backoff.rs` (~100 lines)

**Enhancements**:
- Jitter to prevent thundering herd
- Per-error-type backoff (timeout vs auth failure)
- Exponential backoff with cap (max 5 minutes)
- Early exit for permanent errors (HTTP 4xx)

```rust
pub fn calculate_backoff(
    attempt: u32,
    strategy: BackoffStrategy,
    error: &ActionError,
) -> Duration {
    // Permanent error (4xx)? Don't retry
    if let ActionError::Http(status) = error {
        if status >= 400 && status < 500 {
            return Duration::ZERO;  // No retry
        }
    }

    // Transient error? Retry with backoff
    match strategy {
        BackoffStrategy::Exponential => {
            let base = Duration::from_millis(100);
            let delay = base.mul_f64(2.0_f64.powi(attempt as i32));
            let jitter = Duration::from_millis(rand::random::<u64>() % 100);
            std::cmp::min(delay + jitter, Duration::from_secs(300))
        }
        // ... Linear, Fixed, etc.
    }
}
```

### 3.3 Graceful Degradation
**Files**: Modified `crates/fraiseql-server/src/handler.rs` (~50 lines)

**Features**:
- If Redis unavailable: Queue in-memory (with persistence to disk)
- If ClickHouse unavailable: Buffer events locally
- If Elasticsearch unavailable: Skip indexing (fallback to direct search)
- If one action type failing: Others still work

```rust
impl QueuedObserverExecutor {
    pub async fn process_event(&self, event: &EntityEvent) -> Result<Summary> {
        // Try primary queue (Redis)
        match self.queue.enqueue(job).await {
            Ok(_) => return Ok(Summary::Queued),
            Err(_) if self.fallback_enabled => {
                // Fallback to in-memory queue with persistence
                self.in_memory_queue.push(job);
                self.persist_to_disk(&job);
                return Ok(Summary::DegradedMode);
            }
            Err(e) => return Err(e),
        }
    }
}
```

### 3.4 Error Handling
**Files**: Modified `crates/fraiseql-core/src/error.rs` (~30 lines)

**New error types**:
```rust
pub enum ObserverError {
    // Transient (retry)
    Timeout { message: String },
    NetworkError { message: String },
    DatabaseBusy { message: String },

    // Permanent (don't retry)
    Unauthorized { message: String },
    NotFound { message: String },
    ValidationError { message: String },

    // Circuit breaker
    CircuitBreakerOpen { service: String },
    ServiceDegraded { service: String },
}

impl ObserverError {
    pub fn is_retryable(&self) -> bool {
        matches!(self, Timeout | NetworkError | DatabaseBusy)
    }
}
```

### Tests
- [ ] Circuit breaker transitions correctly (Closed ‚Üí Open ‚Üí HalfOpen ‚Üí Closed)
- [ ] Permanent errors not retried
- [ ] Exponential backoff with jitter works
- [ ] Graceful degradation activates when Redis down
- [ ] In-memory queue persists during Redis outage

### Verification
- [ ] Chaos test: Kill Redis, verify in-memory fallback
- [ ] Load test: Circuit breaker prevents cascading failures
- [ ] Backoff test: Jitter prevents thundering herd

---

## Phase 10.4: Performance Optimization & Profiling (2 days)

### Objective
Achieve target performance benchmarks (15-50x Arrow vs HTTP).

### 4.1 Profiling
**Tools**: Flamegraph, Criterion benchmarks

**What to profile**:
- Event ‚Üí rule matching latency
- Rule ‚Üí action dispatch latency
- Arrow conversion latency
- JSON serialization overhead

### 4.2 Optimization Targets
- Connection pooling (PostgreSQL, Redis, ClickHouse)
- Query plan caching
- Rule compilation caching
- Parallel rule evaluation
- Batch processing (multiple events per operation)

### Tests
- [ ] Flamegraph shows no surprises
- [ ] Arrow performance: >15x vs HTTP baseline
- [ ] Event latency: <100ms p95

### Verification
- [ ] Run benchmarks: `cargo bench`
- [ ] Compare: baseline vs optimized
- [ ] Document: bottlenecks and optimization strategies

---

## Phase 10.5: Complete Authentication & Enhance Authorization (2 days) üü° MOSTLY DONE

### Status: 85% Complete ‚úÖ
**Already Implemented** (2,100+ LOC):
- ‚úÖ JWT validation (HS256, RS256, RS384, RS512) - `crates/fraiseql-core/src/security/auth_middleware.rs` (1,480 LOC)
- ‚úÖ OAuth2/OIDC provider - `crates/fraiseql-server/src/auth/oidc_provider.rs` (342 LOC)
- ‚úÖ Session management with refresh tokens - `crates/fraiseql-server/src/auth/session.rs` (384 LOC)
- ‚úÖ Auth HTTP handlers (start, callback, refresh, logout) - `crates/fraiseql-server/src/auth/handlers.rs` (242 LOC)
- ‚úÖ Auth middleware with Bearer token extraction - `crates/fraiseql-server/src/auth/middleware.rs` (232 LOC)
- ‚úÖ Field-level access control (scope-based) - `crates/fraiseql-core/src/security/field_filter.rs` (752 LOC)
- ‚úÖ Field masking for PII/sensitive data - `crates/fraiseql-core/src/security/field_masking.rs` (532 LOC)
- ‚úÖ Security profiles (Standard vs Regulated) - `crates/fraiseql-core/src/security/profiles.rs` (513 LOC)
- ‚úÖ Audit logging with user tracking - `crates/fraiseql-core/src/security/audit.rs` (222 LOC)

### Objective
Complete OAuth integrations and add operation-level RBAC (mutations).

### 5.1 OAuth Provider Integration (ALREADY 60% DONE)
**Existing**: `crates/fraiseql-server/src/auth/oidc_provider.rs` (342 LOC)

**What's done**:
- Generic OIDC provider trait supporting any OIDC service
- Authorization code flow
- Token refresh
- Token revocation
- PKCE support

**What needs finishing** (1 day):
- GitHub OAuth specific client (wrapper around OIDC)
- Google OAuth specific client (wrapper around OIDC)
- Keycloak integration (group mapping to roles)
- Azure AD integration (app roles mapping)
- Provider factory for configuration-driven setup

```rust
// Existing OIDC provider - just needs provider-specific wrappers
pub struct OidcProvider {
    client_id: String,
    client_secret: String,
    discovery_url: String,  // Auto-discovers endpoints
}

// NEW: Provider-specific wrappers
pub struct GitHubOAuth { /* wraps OidcProvider */ }
pub struct GoogleOAuth { /* wraps OidcProvider */ }
pub struct KeycloakOAuth { /* wraps OidcProvider */ }
```

### 5.2 JWT & Session Management (ALREADY COMPLETE ‚úÖ)
**Existing**:
- `crates/fraiseql-server/src/auth/jwt.rs` (282 LOC) - validation & claims parsing
- `crates/fraiseql-server/src/auth/session.rs` (384 LOC) - session store
- `crates/fraiseql-server/src/auth/session_postgres.rs` (200 LOC) - PostgreSQL backend

**Status**: Production-ready, no changes needed.

### 5.3 Operation-Level RBAC (NEEDS IMPLEMENTATION)
**Existing**: Field-level access control only
**Needed**: Mutation authorization (create/update/delete operations)

**Files**: `crates/fraiseql-server/src/auth/operation_rbac.rs` (NEW, ~200 lines)

```rust
pub enum MutationPermission {
    // Observer rules
    CreateRule,
    UpdateRule,
    DeleteRule,
    // Actions
    ExecuteAction,
    // Admin
    ManageUsers,
    ManageOrgQuota,
}

pub struct OperationPolicy {
    role: Role,
    permissions: HashMap<String, Vec<MutationPermission>>,
}

// Usage: Check permission before mutation
pub fn require_mutation_permission(
    user: &AuthenticatedUser,
    resource: &str,
    action: MutationPermission,
) -> Result<()> {
    let policy = OperationPolicy::for_user(user);
    if !policy.has_permission(resource, action) {
        return Err(ForbiddenError::MutationNotAllowed.into());
    }
    Ok(())
}
```

**Roles**:
- **Admin**: All operations
- **Operator**: Create/update/execute actions, but not delete rules
- **Viewer**: Read-only, no mutations
- **Custom roles**: Define in configuration

### 5.4 API Keys (NEW)
**Files**: `crates/fraiseql-server/src/auth/api_key.rs` (NEW, ~150 lines)

**Features**:
- Create API keys for service-to-service auth
- Key scoping (read-only vs full access)
- Expiration policy (90 day rotation)
- Rate limiting per key

```rust
pub struct ApiKey {
    id: String,
    secret_hash: String,  // Never store plaintext
    scopes: Vec<String>,  // e.g., ["read:rules", "execute:actions"]
    expires_at: DateTime,
    last_used: DateTime,
    created_by: String,   // Audit trail
}

pub struct ApiKeyStore {
    db: Database,  // Persist to PostgreSQL
}

impl ApiKeyStore {
    pub async fn create(&self, key: ApiKey) -> Result<String> {
        // Return base64(key_id:secret) once, never again
    }
    pub async fn validate(&self, key_string: &str) -> Result<ApiKey> {
        // Hash and lookup secret_hash in database
    }
}
```

### 5.5 Integration with Existing Auth
**Files**: Modified `crates/fraiseql-server/src/auth/middleware.rs` (~50 lines)

```rust
// Enhanced middleware: supports JWT + API keys
pub async fn auth_middleware(req: HttpRequest, next: Next) -> Result<HttpResponse> {
    let auth_header = req.headers().get("Authorization")?;

    // Try JWT (Bearer token)
    if let Some(token) = extract_bearer_token(&auth_header) {
        let claims = jwt_validator.validate(token)?;
        let user = AuthenticatedUser::from_claims(claims);
        req.extensions_mut().insert(user);
        return Ok(next.call(req).await);
    }

    // Try API key (Bearer key)
    if let Some(key) = extract_api_key(&auth_header) {
        let api_key = api_key_store.validate(key).await?;
        let user = AuthenticatedUser::from_api_key(api_key);
        req.extensions_mut().insert(user);
        return Ok(next.call(req).await);
    }

    Err(UnauthorizedError.into())
}
```

### 5.6 Configuration
**Files**: Modified `crates/fraiseql-server/src/config.rs` (~30 lines, OIDC part exists)

```toml
[auth]
enabled = true

# OAuth2/OIDC (OIDC provider already implemented)
oauth_provider = "github"  # or "google", "keycloak", "azure_ad"
oidc_discovery_url = "https://accounts.google.com/.well-known/openid-configuration"
oauth_client_id = "..."
oauth_client_secret = "..."

# JWT (already implemented)
jwt_secret = "..."  # From env or Vault
jwt_expiry_minutes = 15
refresh_token_expiry_days = 7

# RBAC (field-level already exists)
# Operation-level needs new config
default_role = "viewer"
admin_users = ["admin@example.com"]
```

### What's Already Working
```rust
// Existing: Field-level access control
pub fn can_read_field(
    user: &AuthenticatedUser,
    type_name: &str,
    field_name: &str,
) -> bool {
    // Scope format: read:User.salary, read:User.*, admin
}

// Existing: Field masking
pub fn apply_field_masking(
    value: &str,
    sensitivity: FieldSensitivity,
) -> String {
    match sensitivity {
        Public => value.to_string(),
        Sensitive => mask_email(value),  // u***
        PII => mask_pii(value),           // PII ****
        Secret => "****".to_string(),
    }
}
```

### Tests
- [ ] GitHub OAuth integration works
- [ ] Google OAuth integration works
- [ ] JWT validation rejects invalid tokens (already tested)
- [ ] API key validation and scoping works
- [ ] Operation RBAC enforces mutation permissions
- [ ] Field-level access control still works (regression)

### Verification
- [ ] `cargo clippy` clean
- [ ] `cargo test auth*` passes
- [ ] HTTP 401 on missing auth
- [ ] HTTP 403 on insufficient permissions
- [ ] API key scopes limit operations
- [ ] Audit logs record who did what (already works)

---

## Phase 10.6: Multi-Tenancy & Data Isolation (2 days) üü° PARTIALLY DONE

### Status: 30% Complete (Data Model) ‚ö†Ô∏è
**Already Implemented**:
- ‚úÖ Tenant ID field in audit logs - `crates/fraiseql-core/src/security/audit.rs` (222 LOC)
- ‚úÖ Tenant/org ID recognized in validation - `crates/fraiseql-core/src/validation/input_processor.rs`
- ‚úÖ JWT claims can include org_id/tenant_id - extracted in `crates/fraiseql-server/src/auth/middleware.rs`

**NOT YET Implemented**:
- ‚ùå Request context enrichment with tenant_id
- ‚ùå Query-level isolation enforcement
- ‚ùå Separate storage layer per tenant (ClickHouse views, Elasticsearch indices)
- ‚ùå Job queue isolation
- ‚ùå Quota enforcement per tenant

### Objective
Enforce strict data isolation between organizations at query execution level.

### 6.1 Request Context Enrichment (HIGHEST PRIORITY)
**Files**: Modified `crates/fraiseql-server/src/logging.rs` (~30 lines)

**Current**:
```rust
pub struct RequestContext {
    pub request_id: RequestId,
    pub operation: Option<String>,
    pub user_id: Option<String>,
    pub client_ip: Option<String>,
    pub api_version: Option<String>,
    // ‚ùå MISSING: tenant_id, org_id, roles
}
```

**Enhanced**:
```rust
pub struct RequestContext {
    pub request_id: RequestId,
    pub user_id: Option<String>,
    pub org_id: Option<String>,      // ‚Üê NEW: From JWT claims.org_id
    pub tenant_id: Option<String>,   // ‚Üê NEW: Alias for org_id if using different naming
    pub roles: Vec<String>,           // ‚Üê NEW: From JWT claims.roles
    pub client_ip: Option<String>,
    pub api_version: Option<String>,
}

// Middleware extracts org_id from JWT
pub async fn tenant_context_middleware(req: HttpRequest, next: Next) -> Result<HttpResponse> {
    let claims = req.extensions().get::<Claims>().ok_or(Unauthorized)?;

    // Add tenant context to request
    let ctx = RequestContext {
        request_id: generate_request_id(),
        user_id: Some(claims.sub.clone()),
        org_id: claims.extra.get("org_id")
            .and_then(|v| v.as_str())
            .map(|s| s.to_string()),
        tenant_id: claims.extra.get("tenant_id")
            .and_then(|v| v.as_str())
            .map(|s| s.to_string()),
        roles: extract_roles(&claims),  // ["admin"], ["operator"], etc.
        client_ip: Some(get_client_ip(&req)),
        api_version: None,
    };

    req.extensions_mut().insert(ctx);
    Ok(next.call(req).await)
}
```

### 6.2 Query-Level Isolation Enforcement
**Files**: `crates/fraiseql-core/src/tenant.rs` (NEW, ~200 lines)

**Tenant filter middleware for all database queries**:

```rust
pub struct TenantFilter {
    org_id: String,
    user_id: String,
}

impl TenantFilter {
    pub fn apply_filter(&self, query: &mut SqlQuery) -> Result<()> {
        // Add `WHERE org_id = $N` to all queries
        query.add_filter("org_id", &self.org_id)?;
        Ok(())
    }
}

// Usage: Wrap all database operations
pub async fn get_rule(&self, ctx: &RequestContext, rule_id: Uuid) -> Result<ObserverRule> {
    let org_id = ctx.org_id.as_ref().ok_or(MissingOrgId)?;

    // Query automatically includes org_id filter
    let rule = sqlx::query_as::<_, ObserverRule>(
        "SELECT * FROM observer_rules WHERE id = $1 AND org_id = $2"
    )
    .bind(rule_id)
    .bind(org_id)  // Cannot be bypassed - comes from JWT
    .fetch_one(&self.pool)
    .await?;
    Ok(rule)
}
```

**For GraphQL queries** (applies to all field resolvers):
```rust
// Every resolver receives context with org_id
pub async fn observer_rules(
    &self,
    ctx: &RequestContext,
) -> Result<Vec<ObserverRule>> {
    let org_id = ctx.org_id.as_ref().ok_or(MissingOrgId)?;

    // All rules filtered by org_id automatically
    self.db.query("SELECT * FROM observer_rules WHERE org_id = $1")
        .bind(org_id)
        .fetch_all()
        .await
}
```

### 6.3 Storage Layer Isolation
**Files**: `crates/fraiseql-arrow/src/tenant.rs` (NEW, ~100 lines)

**ClickHouse views per organization**:
```sql
-- Main table (all orgs, partitioned by org)
CREATE TABLE fraiseql_events (
    event_id UUID,
    org_id UUID,
    timestamp DateTime,
    data String,
    ...
) ENGINE = MergeTree()
PARTITION BY org_id
ORDER BY (org_id, timestamp);

-- Optional: Per-org materialized views for performance
CREATE MATERIALIZED VIEW fraiseql_events_org_123_mv AS
SELECT * FROM fraiseql_events WHERE org_id = '123-456...'
```

**Elasticsearch indices per organization** (optional, for operational search):
```json
{
  "fraiseql-events-org-123": {
    "settings": {
      "number_of_replicas": 2,
      "index.lifecycle.name": "fraiseql-policy"
    }
  },
  "fraiseql-events-org-789": { ... }
}
```

### 6.4 Job Queue Isolation
**Files**: Modified `crates/fraiseql-observers/src/job_queue/redis.rs` (~30 lines)

**Separate Redis queues per organization** (already has org_id in events, just needs routing):
```rust
pub fn queue_key(org_id: &str) -> String {
    format!("fraiseql:queue:org:{}", org_id)
}

// Enqueue: adds to org-specific queue (from context)
pub async fn enqueue(&self, ctx: &RequestContext, job: Job) -> Result<()> {
    let org_id = ctx.org_id.as_ref().ok_or(MissingOrgId)?;
    let key = queue_key(org_id);
    redis_client.lpush(&key, serialize(&job)).await?;
    Ok(())
}

// Dequeue: worker reads only from its org's queue
pub async fn dequeue(&self, org_id: &str, count: usize) -> Result<Vec<Job>> {
    let key = queue_key(org_id);
    redis_client.lrange(&key, 0, count as i64).await
}
```

### 6.5 Quota Enforcement
**Files**: `crates/fraiseql-server/src/quota.rs` (NEW, ~150 lines)

**Per-organization quotas** (stored in database):
```toml
# Configuration profiles for organizations
[quota.default]
max_rules = 100
max_actions_per_rule = 10
max_storage_gb = 100
max_qps = 1000

[quota.enterprise]
max_rules = 10000
max_actions_per_rule = 1000
max_storage_gb = 1000
max_qps = 100000
```

**Runtime quota enforcement**:
```rust
pub struct QuotaManager {
    db: Database,  // Read org quotas from database
}

impl QuotaManager {
    pub async fn check_quota(&self, ctx: &RequestContext, resource: &str) -> Result<()> {
        let org_id = ctx.org_id.as_ref().ok_or(MissingOrgId)?;

        // Get org's quota from database
        let org = self.db.get_organization(org_id).await?;
        let usage = self.db.get_usage(org_id).await?;

        match resource {
            "rules" => {
                if usage.rule_count >= org.quota.max_rules {
                    return Err(QuotaExceeded::Rules.into());
                }
            }
            "qps" => {
                if current_qps() > org.quota.max_qps {
                    return Err(QuotaExceeded::Qps.into());
                }
            }
            _ => {}
        }
        Ok(())
    }
}

// Usage: Check before creating rule
pub async fn create_rule(&self, ctx: &RequestContext, rule: ObserverRule) -> Result<()> {
    quota_manager.check_quota(ctx, "rules").await?;
    // ... create rule
}
```

### 6.6 Audit Logging (ALREADY PARTIALLY DONE ‚úÖ)
**Existing**: `crates/fraiseql-core/src/security/audit.rs` (222 LOC)

**What's done**:
- ‚úÖ Audit log schema includes org_id/tenant_id
- ‚úÖ User tracking (user_id, username)
- ‚úÖ Query logging
- ‚úÖ IP address and user agent tracking

**What needs enhancement** (~20 lines):
- Add mutation tracking (who created/updated/deleted what)
- Add resource identifiers to audit logs
- Connect audit logging to quota enforcement

```rust
// Existing audit structure - just needs connection to mutations
pub struct AuditEntry {
    pub tenant_id: i64,      // ‚úÖ Already there
    pub user_id: i64,        // ‚úÖ Already there
    pub operation: String,   // ‚úÖ Already there
    pub query: String,       // ‚úÖ Already there
    // Need to enhance:
    pub mutation_type: Option<String>,  // create, update, delete
    pub resource_id: Option<String>,    // rule ID, action ID, etc.
}
```

### 6.7 Tenant Initialization
**Database schema changes** (existing tables need org_id):
```sql
-- All existing tables need org_id column
ALTER TABLE observer_rules ADD COLUMN org_id UUID NOT NULL DEFAULT gen_random_uuid();
ALTER TABLE observer_actions ADD COLUMN org_id UUID NOT NULL;
ALTER TABLE audit_logs ADD COLUMN org_id UUID NOT NULL;  -- Already has this

-- Add tenant table for quota/config
CREATE TABLE organizations (
    id UUID PRIMARY KEY,
    name VARCHAR(255),
    quota_tier VARCHAR(50),  -- 'default', 'enterprise', custom
    created_at TIMESTAMP,
    created_by UUID,
);

-- Add indexes for org-filtered queries
CREATE INDEX idx_observer_rules_org_id ON observer_rules(org_id);
CREATE INDEX idx_observer_actions_org_id ON observer_actions(org_id);
CREATE INDEX idx_events_org_id ON fraiseql_events(org_id);
```

### Tests
- [ ] Org A cannot read Org B's rules (query isolation)
- [ ] Org A cannot execute Org B's actions (authorization)
- [ ] Org A jobs isolated in separate queue (queue isolation)
- [ ] Quota exceeded blocks operation (quota enforcement)
- [ ] Audit log records all operations per org (audit trail)
- [ ] Cross-org data access returns empty (security test)

### Verification
- [ ] Run data isolation tests: `cargo test tenant*`
- [ ] Run quota tests: `cargo test quota*`
- [ ] Run audit tests: `cargo test audit*`
- [ ] Manual: Try to access org_id from JWT in every query

---

## Phase 10.7: Distributed Tracing (1-2 days)

### Objective
Trace requests end-to-end for debugging and performance analysis.

### 7.1 OpenTelemetry Integration
**Files**: `crates/fraiseql-server/src/tracing.rs` (NEW, ~150 lines)

```rust
use opentelemetry::{api::KeyValue, sdk::trace as sdktrace, exporter::trace};

pub fn init_tracing() -> Result<()> {
    let exporter = jaeger::new_pipeline()
        .install_simple()
        .context("Failed to initialize Jaeger")?;

    let tracer = opentelemetry::global::tracer("fraiseql");
    Ok(())
}

// Middleware: create span for each request
pub async fn tracing_middleware(req: HttpRequest, next: Next) -> Result<HttpResponse> {
    let tracer = opentelemetry::global::tracer("fraiseql");
    let span = tracer.start("http_request");

    span.set_attribute(KeyValue::new("http.method", req.method().to_string()));
    span.set_attribute(KeyValue::new("http.url", req.uri().to_string()));

    let response = next.call(req).await?;

    span.set_attribute(KeyValue::new("http.status_code", response.status().as_u16() as i64));
    Ok(response)
}
```

### 7.2 Instrumentation Points
**Spans for**:
- HTTP request handling
- Database queries
- Rule evaluation
- Action execution
- Job queue operations

### Tests
- [ ] Traces exported to Jaeger
- [ ] Span hierarchy is correct
- [ ] Attributes are recorded

---

## Phase 10.8: Secrets Management (1-2 days)

### Objective
Secure handling of sensitive configuration (webhook URLs, API keys, tokens).

### 8.1 HashiCorp Vault Integration
**Files**: `crates/fraiseql-server/src/secrets.rs` (NEW, ~200 lines)

```rust
pub struct SecretManager {
    vault_client: VaultClient,
    cache: Arc<Mutex<HashMap<String, Secret>>>,
    cache_ttl: Duration,
}

impl SecretManager {
    pub async fn get_secret(&self, path: &str) -> Result<Secret> {
        // Check cache
        if let Some(secret) = self.cache.lock().await.get(path) {
            if !secret.is_expired() {
                return Ok(secret.clone());
            }
        }

        // Fetch from Vault
        let secret = self.vault_client.read(path).await?;
        self.cache.lock().await.insert(path.to_string(), secret.clone());
        Ok(secret)
    }
}
```

### 8.2 Configuration
**No secrets in TOML**:
```toml
[webhook_actions.example]
# ‚ùå Wrong: hardcoded secret
webhook_url = "https://hooks.example.com/SECRET123"

# ‚úÖ Right: reference to Vault
webhook_url_secret = "vault://webhook-urls/example"
```

**Bootstrap**:
```bash
# Register secrets with Vault before starting
vault kv put secret/fraiseql/webhook-urls/example \
  url="https://hooks.example.com/SECRET123"

# Start server (reads from Vault)
VAULT_ADDR=http://vault:8200 \
VAULT_TOKEN=s.XXXXXX \
fraiseql-server --config fraiseql.toml
```

### Tests
- [ ] Secret fetching works
- [ ] Cache invalidation works
- [ ] Rotation without restart works

---

## Phase 10.9: Backup & Disaster Recovery (1 day)

### Objective
Ensure data recovery from failures.

### 9.1 Backup Strategy
**Components to backup**:
- PostgreSQL (observer rules, user data)
- Redis (job queue state)
- ClickHouse (event analytics)
- Elasticsearch (operational search indices)

**Backup frequency**:
- PostgreSQL: Hourly snapshots + WAL replication
- Redis: Daily dumps (AOF enabled for persistence)
- ClickHouse: Daily snapshots
- Elasticsearch: Daily snapshots

### 9.2 Recovery Runbook
**Document**:
1. Restore PostgreSQL from hourly backup
2. Restore Redis AOF or dump
3. Restore ClickHouse from snapshot
4. Restore Elasticsearch indices from snapshot
5. Verify data consistency
6. Run acceptance tests

**Expected RTO**: 1 hour
**Expected RPO**: Last hourly backup (max 1 hour data loss)

### 9.3 Disaster Recovery Tests
- Quarterly restore from backup
- Document any issues found
- Update runbook

---

## Phase 10.10: Encryption at Rest & In Transit (1-2 days)

### Objective
Protect data from unauthorized access.

### 10.1 TLS for All Connections

**HTTP/gRPC**:
```toml
[server]
bind_address = "0.0.0.0:8000"
tls_cert_path = "/etc/fraiseql/cert.pem"
tls_key_path = "/etc/fraiseql/key.pem"
# mTLS for Arrow Flight gRPC
require_client_cert = true
client_ca_path = "/etc/fraiseql/client-ca.pem"
```

**Database connections**:
```toml
[database]
url = "postgresql://user:pass@localhost:5432/fraiseql?sslmode=require"

[redis]
url = "rediss://localhost:6379"  # Secure Redis

[clickhouse]
url = "https://localhost:8123"
verify_cert = true
```

### 10.2 At-Rest Encryption
**ClickHouse** (if supported by version):
```sql
CREATE TABLE fraiseql_events (
    ...
) ENGINE = MergeTree()
WITH SETTINGS
    storage_disk_name = 'encrypted';
```

**Elasticsearch** (ILM with encryption):
```json
{
  "policy": "fraiseql-policy",
  "phases": {
    "hot": {
      "min_age": "0d",
      "actions": {
        "rollover": { "max_size": "50gb" }
      }
    }
  }
}
```

### Tests
- [ ] All connections use TLS
- [ ] Certificate validation works
- [ ] At-rest encryption enabled

---

## Summary: Phase 10 Phases at a Glance

| Phase | Status | Effort | Dependencies | Go/No-Go |
|-------|--------|--------|--------------|----------|
| 10.1 | Not started | 3 days | None | üü° Recommended |
| 10.2 | Not started | 2 days | None | üü° Recommended |
| 10.3 | Not started | 3 days | 10.1 | üü° Recommended |
| 10.4 | Not started | 2 days | None | üü¢ Optional (nice-to-have) |
| **10.5** | **85% Done (2,100 LOC)** | **2 days** | **None** | **üü° MOSTLY DONE** |
| **10.6** | **30% Done (Schema only)** | **2 days** | **10.5** | **üü° PRIORITY** |
| 10.7 | Not started | 1-2 days | None | üü° Recommended |
| **10.8** | **Not started** | **1-2 days** | **10.5** | **üî¥ CRITICAL** |
| **10.9** | **Not started** | **1 day** | **None** | **üî¥ CRITICAL** |
| **10.10** | **Not started** | **1-2 days** | **None** | **üî¥ CRITICAL** |

**Revised Critical Path**: 10.5 (finish) ‚Üí 10.6 (enforce) ‚Üí 10.8, 10.9, 10.10
**Realistic Effort**: 2 weeks (vs 3-4 weeks, since 85% of auth already done)

---

## Implementation Order (Revised - 2 Weeks Now That Auth is 85% Done)

```
WEEK 1: Complete Auth & Enforce Multi-Tenancy
‚îú‚îÄ 10.5: Finish OAuth providers + API keys [2 days]
‚îÇ  ‚îî‚îÄ Most JWT/session/OIDC already done, just need provider wrappers
‚îú‚îÄ 10.6: Enforce tenant isolation [2 days]
‚îÇ  ‚îî‚îÄ Add org_id to RequestContext, apply filters to all queries
‚îî‚îÄ 10.1: Rate limiting & admission control [1 day]
   ‚îî‚îÄ Wire org_id into rate limiter

WEEK 2: Operational Hardening
‚îú‚îÄ 10.8: Secrets management (Vault integration) [1-2 days]
‚îú‚îÄ 10.9: Backup & disaster recovery runbook [1 day]
‚îú‚îÄ 10.10: Encryption at rest & transit [1-2 days]
‚îú‚îÄ 10.3: Circuit breakers (optional) [1 day]
‚îî‚îÄ Integration & release prep [1-2 days]
```

**Total Effort**: 2 weeks (vs 3-4 weeks)
**Critical Path**: 10.5 (finish) ‚Üí 10.6 (enforce) ‚Üí 10.8, 10.9, 10.10
**Optional But Nice**: 10.1, 10.2, 10.3, 10.4, 10.7

---

## Success Criteria

- [ ] All Phase 10 tests passing
- [ ] Zero clippy warnings in new code
- [ ] Performance benchmarks met (15-50x Arrow vs HTTP)
- [ ] Security audit passed (no critical issues)
- [ ] Multi-tenant isolation verified
- [ ] Backup/restore tested successfully
- [ ] Documentation complete and accurate
- [ ] Deployment to Kubernetes working

---

**Status**: Ready to begin Phase 10
**Next Step**: Execute Phase 9.9 testing, then start Phase 10.5 (Auth)
**Owner**: You
**Timeline**: 3-4 weeks to production-ready

